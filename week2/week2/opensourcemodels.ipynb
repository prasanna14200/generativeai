{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4dfb4c",
   "metadata": {},
   "source": [
    "How to Download & ##### Run Open Source Models in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5228c",
   "metadata": {},
   "source": [
    "##### 1. Using Hugging Face transformers directly (recommended for most models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562b17b3",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f9373",
   "metadata": {},
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = snapshot_download(\"EleutherAI/gpt-j-6B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ac2715",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e42f5b",
   "metadata": {},
   "source": [
    "##### 3. Using git-lfs to clone the model repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df005f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://huggingface.co/EleutherAI/gpt-j-6B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acd801",
   "metadata": {},
   "source": [
    "4. Downloading pre-quantized or converted weights from third parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ec8cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://example.com/path/to/quantized-model.bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a86834",
   "metadata": {},
   "source": [
    "5. Mount Google Drive and store models there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe19da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Then download or copy model files to /content/drive/MyDrive/models/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e6c2c",
   "metadata": {},
   "source": [
    "Requires git-lfs installed in Colab (!apt install git-lfs) and set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0b442",
   "metadata": {},
   "source": [
    "| Method                              | Best For                     | Pros                           | Cons                        |\n",
    "| ----------------------------------- | ---------------------------- | ------------------------------ | --------------------------- |\n",
    "| `transformers.from_pretrained`      | Most models, easy            | Simple, automatic download     | Slow startup, requires net  |\n",
    "| `huggingface_hub.snapshot_download` | Large models, manual control | Cache once, offline use        | Manual work, disk space     |\n",
    "| `git-lfs` clone                     | Full repo control            | Inspect files, customize       | Setup overhead, slower      |\n",
    "| Manual wget/download (quantized)    | Optimized, small models      | Fast, low memory usage         | Compatibility, manual steps |\n",
    "| Google Drive mount                  | Persistent storage           | No redownloading every session | Slower IO                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff00d9",
   "metadata": {},
   "source": [
    "| Model Type                                                  | API Key Required? | Why?                                           |\n",
    "| ----------------------------------------------------------- | ----------------- | ---------------------------------------------- |\n",
    "| Hosted API services (OpenAI, Anthropic, Google Gemini)      | Yes               | Company runs model on cloud, paid/gated access |\n",
    "| Gated private models on Hugging Face (some LLaMA 3, etc.)   | Yes               | Restricted access requires login/token         |\n",
    "| Fully open-source downloadable models (GPT-J, Falcon, etc.) | No                | You run model locally or on your own server    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b2b1b",
   "metadata": {},
   "source": [
    "| Access Type                                    | Famous Examples                                                                                                                                                             | Description / Notes                                                                                                                              |\n",
    "| ---------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Hosted API (API key needed)**                | - OpenAI GPT-4, GPT-3<br>- Anthropic Claude<br>- Google Bard / Gemini (planned)<br>- Cohere<br>- AI21 Labs Jurassic                                                         | Models run on cloud servers; require API key for usage; mostly paid or limited free tiers.                                                       |\n",
    "| **Gated Private Models (access token needed)** | - Meta LLaMA 3<br>- Some Falcon 40B versions<br>- GPT-NeoX 20B (certain releases)<br>- Claude 2 (Anthropic gated)<br>- Some proprietary Hugging Face repos                  | Access requires approval, license agreement, or token; often not fully open-source; weights may not be publicly downloadable without permission. |\n",
    "| **Fully Open Source (no API key needed)**      | - GPT-J 6B (EleutherAI)<br>- GPT-Neo 2.7B<br>- Falcon 7B & 40B (OpenAccessAI)<br>- LLaMA 2 (Meta, partially open)<br>- Dolly 2.0 (Databricks)<br>- Mistral 7B<br>- StableLM | Model weights and code freely downloadable; you can run locally or in Colab without API keys; license permitting usage.                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c596402",
   "metadata": {},
   "source": [
    "1️⃣ Hosted API Model Example: OpenAI GPT-4 / GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a39e4",
   "metadata": {},
   "source": [
    "Step-by-step to use OpenAI GPT in Colab:\n",
    "Get OpenAI API key:\n",
    "\n",
    "Sign up at OpenAI.\n",
    "\n",
    "Get your API key from your dashboard.\n",
    "\n",
    "Install OpenAI Python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29dff50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4a0b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Replace with your actual OpenAI API key\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",  # or \"gpt-3.5-turbo\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, who won the world cup in 2022?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9ed67",
   "metadata": {},
   "source": [
    "2️⃣ Gated Private Model Example: Meta LLaMA 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09580254",
   "metadata": {},
   "source": [
    "Step-by-step to use gated LLaMA 3 model in Colab:\n",
    "Request access:\n",
    "\n",
    "Go to Meta LLaMA page on Hugging Face.\n",
    "\n",
    "Apply and agree to terms to get access.\n",
    "\n",
    "Get your Hugging Face token:\n",
    "\n",
    "Sign in on Hugging Face.\n",
    "\n",
    "Go to your settings > Access Tokens > create new token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b4173",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b005ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"YOUR_HUGGINGFACE_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd9108",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"  # Example (for LLaMA 2, 3 may be similar)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "inputs = tokenizer(\"Hello LLaMA!\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc65f2a",
   "metadata": {},
   "source": [
    "3️⃣ Fully Open Source Model Example: GPT-J 6B (EleutherAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368eb7f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fcb30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "inputs = tokenizer(\"What is the capital of France?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
